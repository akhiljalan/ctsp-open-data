{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhiljalan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import errno\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "\n",
    "\n",
    "\n",
    "def maybe_download(url, local_dir, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    mkdir_p(local_dir)\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filepath = os.path.join(local_dir, local_filename)\n",
    "    if not os.path.exists(local_filepath):\n",
    "        print(\"Downloading %s...\" % local_filename)\n",
    "        local_filename, _ = urllib.request.urlretrieve(url,\n",
    "                                                       local_filepath)\n",
    "        print(\"Finished!\")\n",
    "    statinfo = os.stat(local_filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', local_filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + local_filename +\n",
    "                        '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    \"\"\"From https://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"Create a session that dynamically allocates memory.\"\"\"\n",
    "    # See: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# maybe_download('http://nlp.stanford.edu/data/glove.6B.zip', 'datasets', 862182613)\n",
    "# if not os.path.exists(os.path.join(\"datasets\", \"glove.6B.50d.txt\")):\n",
    "#     with zipfile.ZipFile(os.path.join(\"datasets\", \"glove.6B.zip\"), \"r\") as zip_ref:\n",
    "#         zip_ref.extractall(\"datasets\")\n",
    "#     for f in [\"glove.6B.100d.txt\", \"glove.6B.300d.txt\", \"glove.6B.200d.txt\"]:\n",
    "#         os.remove(os.path.join('datasets', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 50-dimensional embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    vocab = []\n",
    "    embed = []\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab.append(row[0])\n",
    "            embed.append(row[1:])\n",
    "    embed = np.asarray(embed)\n",
    "    return vocab, embed\n",
    "\n",
    "\n",
    "# Load the GloVe vectors into numpy\n",
    "# glove_filepath = os.path.join('datasets', 'glove.6B.50d.txt')\n",
    "glove_filepath = '/Users/akhiljalan/Desktop/glove.6B.50d.txt'\n",
    "vocab, embed = load_embeddings(glove_filepath)\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = len(embed[0])\n",
    "assert vocab_size > 0, \"The vocabulary shouldn't be empty; did you download the GloVe weights?\"\n",
    "print('Loaded %d %d-dimensional embeddings.' % (vocab_size, embed_dim))\n",
    "\n",
    "# word2id = {}\n",
    "# id2word = vocab\n",
    "# for i, w in enumerate(id2word):\n",
    "#     word2id[w] = i\n",
    "\n",
    "# Ops to load the embeddings into TensorFlow\n",
    "# embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embed_dim]),\n",
    "#                         trainable=False, name=\"embedding\")\n",
    "# embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embed_dim])\n",
    "# embedding_init = embedding.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO pick general categories later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_words(words):\n",
    "    word_embeds = []\n",
    "    final_words = []\n",
    "    for i in range(400000):\n",
    "        for word in words:\n",
    "            if word == vocab[i]: \n",
    "#                 print('Word {} is in the dataset'.format(vocab[i]))\n",
    "                final_words.append(word)\n",
    "                word_embeds.append(embed[i])\n",
    "                words.remove(vocab[i])\n",
    "    return final_words, word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matricize_embed_list(indiv_embeds):\n",
    "        embeds = np.vstack(tuple(indiv_embeds)).astype(np.float32)\n",
    "        row_norms = np.repeat(np.expand_dims(1 / np.linalg.norm(embeds, axis=1), 1), \n",
    "                              repeats=len(embeds[0]), axis=1)\n",
    "        normalized_embeds = embeds * row_norms\n",
    "        return normalized_embeds\n",
    "    \n",
    "def load_category_words(category_word_list):\n",
    "    category_words, indiv_embeds = get_embed_for_words(category_word_list)\n",
    "    normalized_cat_embeds = matricize_embed_list(indiv_embeds)\n",
    "    return category_words, normalized_cat_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, embeds = load_category_words(['happy', 'sad', 'death'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_categories = ['police', 'budget', 'meeting', 'election']\n",
    "# category_words, category_embeds = get_embed_for_words(general_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_repeats(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        all_cat = f.readline().split(',')\n",
    "    return all_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_phrases = load_with_repeats('../categories.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words = []\n",
    "for phrase in tag_phrases: \n",
    "    for x in phrase.strip().split(' '):\n",
    "        tag_words.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words, tag_embeddings = get_embed_for_words(tag_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embeds = matricize_embed_list(tag_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 50)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tag_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 91)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(category_embeds, tag_embed_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_sim_scores(category_embeds, word_embeds): \n",
    "    sim_matrix = np.matmul(category_embeds, word_embeds)\n",
    "    masked_sim_matrix = sim_matrix * (sim_matrix >= 0)\n",
    "    return np.sum(masked_sim_matrix, axis = 1) / sim_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarites(category_word_list, tag_word_list): \n",
    "    # get embedding matrices\n",
    "    cat_words, cat_embeds = get_embed_for_words(category_word_list)\n",
    "    cat_embed_matrix = matricize_embed_list(cat_embeds)\n",
    "    tag_words, tag_embeds = get_embed_for_words(tag_word_list)\n",
    "    tag_embed_matrix = matricize_embed_list(tag_embeds)\n",
    "    \n",
    "    # return category sim scores \n",
    "    sim_scores = category_sim_scores(cat_embed_matrix, tag_embed_matrix.T)\n",
    "    for word, score in zip(cat_words, sim_scores): \n",
    "        print('Similitarity to {}: {}'.format(word.upper(), score))\n",
    "    # need both matrices\n",
    "    # print the words on top!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_similarities(category_word_list_of_lists, tag_word_list): \n",
    "    # tag words are \"content\" words\n",
    "    '''\n",
    "    Input: \n",
    "    category_word_list_of_lists: List of lists of strings. \n",
    "    Each list corresponds to a number of related category words, such as \n",
    "    ['park', 'forest', 'nature']\n",
    "    \n",
    "    tag_word_list: List of strings. These are the datasets we've scraped. \n",
    "    \n",
    "    Returns: \n",
    "    final_cats_nested: A list of lists, which should be identical to category_word_list_of_lists\n",
    "    in the ideal case. Some of the inner lists might not contain all the original words, because \n",
    "    GloVe doesn not have an embedding for every word in the English language. \n",
    "    \n",
    "    final_scores: Numerical scores between [0, 1] for each category-list. \n",
    "    Computed as an average over all the tags in the tag_word_list. \n",
    "    '''\n",
    "    tag_words, tag_embeds = get_embed_for_words(tag_word_list)\n",
    "    tag_embed_matrix = matricize_embed_list(tag_embeds)\n",
    "    final_scores = []\n",
    "    final_cats_nested = []\n",
    "    for cat_word_list in category_word_list_of_lists: \n",
    "        cat_words, cat_embeds = get_embed_for_words(cat_word_list)\n",
    "        cat_embed_matrix = matricize_embed_list(cat_embeds)\n",
    "        sim_scores = category_sim_scores(cat_embed_matrix, tag_embed_matrix.T)\n",
    "        final_scores.append(np.mean(sim_scores))\n",
    "        final_cats_nested.append(cat_words)\n",
    "        print('Similarity to all words {}: {}'.format(cat_words, final_scores[-1]))\n",
    "    return final_cats_nested, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['police',\n",
       " 'police',\n",
       " 'age',\n",
       " 'race',\n",
       " 'energy',\n",
       " 'gas',\n",
       " 'data',\n",
       " 'green',\n",
       " 'natural',\n",
       " 'crime',\n",
       " 'housing',\n",
       " 'census',\n",
       " 'vehicle',\n",
       " 'enforcement',\n",
       " 'electricity']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(tag_words))\n",
    "nested_cats = [['safety', 'police'], ['school', 'pupil', 'teacher']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity to all words ['police', 'safety']: 0.44999271631240845\n",
      "Similarity to all words ['school', 'teacher', 'pupil']: 0.2811836004257202\n"
     ]
    }
   ],
   "source": [
    "category_lists, scores = compute_categorical_similarities(nested_cats, list(set(tag_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all arrays must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e2e9efc017c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 385\u001b[0;31m                                              copy=copy)\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_axes\u001b[0;34m(N, K, index, columns)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_ensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   4200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall_arrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4201\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4204\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all arrays must be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_factorize_from_iterables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all arrays must be same length"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(scores, columns=category_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitarity to DEATH: 0.3337719440460205\n",
      "Similitarity to HAPPY: 0.24990543723106384\n",
      "Similitarity to SAD: 0.15696865320205688\n"
     ]
    }
   ],
   "source": [
    "whole_thing(['happy', 'sad', 'death'], tag_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34142044, 0.2540078 , 0.15936016], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_sim_scores(embeds, tag_embeds.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.matmul(normalized_category_embeds, normalized_tag_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  0.37782702,\n",
       "         0.50888675,  0.51192677,  0.36461431,  0.33621722,  0.27145368,\n",
       "         0.27145368,  0.3727001 ,  0.47038737,  0.28251207,  0.28251207,\n",
       "         0.54297966,  0.34372619,  0.20126359,  0.20126359,  0.66294223,\n",
       "         0.57497567,  0.22185263,  0.62788248,  0.3836084 ,  0.55254734,\n",
       "         0.39194855,  0.39194855,  0.39194855,  0.5385704 ,  0.34456822,\n",
       "         0.34456822,  0.32892108,  0.32892108,  0.19693632,  0.30481961,\n",
       "         0.21267366,  0.21267366,  0.21267366,  0.44482815,  0.62654668,\n",
       "         0.62654668,  0.62654668,  0.38475591,  0.38475591,  0.56387287,\n",
       "         0.68423003,  0.37948063,  0.5721826 ,  0.33391023,  0.33391023,\n",
       "         0.58605242,  0.58605242,  0.12189772,  0.32167047,  0.38043648,\n",
       "         0.43361136,  0.41414142,  0.68039668,  0.68039668,  0.27552515,\n",
       "         0.27552515,  0.27552515,  0.19996311,  0.68338013,  0.25361446,\n",
       "         0.38162985,  0.56028682,  0.65576679,  0.37025639,  0.11422724,\n",
       "         0.29947293,  0.19687355,  0.27971968,  0.30604336,  0.51360476,\n",
       "         0.15534474,  0.26744708,  0.60964477,  0.25539339,  0.30165786,\n",
       "         0.29863361,  0.5266009 ,  0.28536004, -0.        ,  0.43060783,\n",
       "         0.0177402 , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.43742308,  0.43742308,  0.43742308,  0.43742308,  0.52838868,\n",
       "         0.42905241,  0.46678743,  0.65096939,  0.30177182,  0.24810165,\n",
       "         0.24810165,  0.24861616,  0.5116722 ,  0.41473097,  0.41473097,\n",
       "         0.55864531,  0.4802483 ,  0.43790609,  0.43790609,  0.57097733,\n",
       "         0.43917152,  0.52943707,  0.46986008,  0.47619838,  0.32804558,\n",
       "         0.29981244,  0.29981244,  0.29981244,  0.49528596,  0.30161899,\n",
       "         0.30161899,  0.36774021,  0.36774021,  0.19299115,  0.20983514,\n",
       "         0.281634  ,  0.281634  ,  0.281634  ,  0.49841818,  0.30070871,\n",
       "         0.30070871,  0.30070871,  0.31955966,  0.31955966,  0.40493912,\n",
       "         0.31785494,  0.33014095,  0.28882712,  0.2245751 ,  0.2245751 ,\n",
       "         0.18902121,  0.18902121,  0.52138072,  0.20762987,  0.26687431,\n",
       "         0.39733282,  0.36077687,  0.38452768,  0.38452768,  0.23440462,\n",
       "         0.23440462,  0.23440462,  0.25426605,  0.19790871,  0.34248236,\n",
       "         0.33473939,  0.32937309,  0.32809025,  0.12042347,  0.22253926,\n",
       "         0.28812543,  0.19473554,  0.23323674,  0.29746073,  0.01549328,\n",
       "         0.13253495,  0.33884454,  0.05585386,  0.05835186,  0.25203559,\n",
       "         0.25820723,  0.17902988,  0.02417327, -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.3963829 ,  0.3963829 ,  0.3963829 ,  0.3963829 ,  0.38452652,\n",
       "         0.51313835,  0.37405446,  0.56275493,  0.28455937,  0.35337049,\n",
       "         0.35337049,  0.37067395,  0.39031026,  0.62583321,  0.62583321,\n",
       "         0.59972864,  0.42328104,  0.36528972,  0.36528972,  0.5055874 ,\n",
       "         0.52446562,  0.59329444,  0.43586317,  0.29828978,  0.4019224 ,\n",
       "         0.26624027,  0.26624027,  0.26624027,  0.58451641,  0.33856285,\n",
       "         0.33856285,  0.33490443,  0.33490443,  0.36411482,  0.31653881,\n",
       "         0.24491249,  0.24491249,  0.24491249,  0.53633106,  0.44441333,\n",
       "         0.44441333,  0.44441333,  0.30137742,  0.30137742,  0.32086575,\n",
       "         0.42658946,  0.29734758,  0.33642688,  0.46806383,  0.46806383,\n",
       "         0.27297479,  0.27297479,  0.44001964,  0.24368942,  0.3520593 ,\n",
       "         0.33278573,  0.40744418,  0.3661918 ,  0.3661918 ,  0.34401673,\n",
       "         0.34401673,  0.34401673,  0.15076286,  0.34962896,  0.55429977,\n",
       "         0.35896578,  0.24775165,  0.39736819,  0.19988504,  0.30693039,\n",
       "         0.17992707,  0.31857392,  0.37470382,  0.32195592,  0.21389291,\n",
       "         0.20530343,  0.32797876,  0.17473499,  0.08561238,  0.21350507,\n",
       "         0.15347581,  0.10677678,  0.04662498,  0.06661593,  0.11121931,\n",
       "         0.05594183, -0.        ,  0.12720963, -0.        , -0.        ,\n",
       "        -0.        ],\n",
       "       [ 0.22185263,  0.22185263,  0.22185263,  0.22185263,  0.56659418,\n",
       "         0.47102541,  0.47677842,  0.77362508,  0.33774829,  0.30845928,\n",
       "         0.30845928,  0.36815166,  0.31406945,  0.31150037,  0.31150037,\n",
       "         0.60234165,  0.52724814,  0.476509  ,  0.476509  ,  0.45816207,\n",
       "         0.42724708,  0.99999982,  0.40277794,  0.54418135,  0.3993533 ,\n",
       "         0.34464106,  0.34464106,  0.34464106,  0.38629368,  0.4424164 ,\n",
       "         0.4424164 ,  0.32540479,  0.32540479,  0.59629846,  0.25441056,\n",
       "         0.3353557 ,  0.3353557 ,  0.3353557 ,  0.47109085,  0.48841587,\n",
       "         0.48841587,  0.48841587,  0.59075952,  0.59075952,  0.3171736 ,\n",
       "         0.33356491,  0.49298671,  0.3961294 ,  0.37600589,  0.37600589,\n",
       "         0.33260027,  0.33260027,  0.5500772 ,  0.2277188 ,  0.44385132,\n",
       "         0.52390075,  0.48812655,  0.46085599,  0.46085599,  0.42947978,\n",
       "         0.42947978,  0.42947978,  0.16127183,  0.18022741,  0.52799422,\n",
       "         0.31432983,  0.37027586,  0.28414345,  0.14516728,  0.46411312,\n",
       "         0.3910799 ,  0.2883862 ,  0.18890667,  0.5956316 ,  0.19338171,\n",
       "         0.17646202,  0.26442659,  0.09401111,  0.1233243 ,  0.18958509,\n",
       "         0.25366941,  0.29910392,  0.10134585,  0.248937  ,  0.00943356,\n",
       "         0.26597464, -0.        , -0.        ,  0.23767906, -0.        ,\n",
       "        -0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sim_matrix = sim_matrix * (sim_matrix >= 0)\n",
    "masked_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34174143613039792"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(masked_sim_matrix)/(masked_sim_matrix.shape[0] * masked_sim_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(np.random.randn(5, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
