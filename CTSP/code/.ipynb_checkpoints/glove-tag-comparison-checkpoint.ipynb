{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhiljalan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import errno\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "\n",
    "\n",
    "\n",
    "def maybe_download(url, local_dir, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    mkdir_p(local_dir)\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filepath = os.path.join(local_dir, local_filename)\n",
    "    if not os.path.exists(local_filepath):\n",
    "        print(\"Downloading %s...\" % local_filename)\n",
    "        local_filename, _ = urllib.request.urlretrieve(url,\n",
    "                                                       local_filepath)\n",
    "        print(\"Finished!\")\n",
    "    statinfo = os.stat(local_filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', local_filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + local_filename +\n",
    "                        '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    \"\"\"From https://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"Create a session that dynamically allocates memory.\"\"\"\n",
    "    # See: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified datasets/glove.6B.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "maybe_download('http://nlp.stanford.edu/data/glove.6B.zip', 'datasets', 862182613)\n",
    "if not os.path.exists(os.path.join(\"datasets\", \"glove.6B.50d.txt\")):\n",
    "    with zipfile.ZipFile(os.path.join(\"datasets\", \"glove.6B.zip\"), \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"datasets\")\n",
    "    for f in [\"glove.6B.100d.txt\", \"glove.6B.300d.txt\", \"glove.6B.200d.txt\"]:\n",
    "        os.remove(os.path.join('datasets', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 50-dimensional embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    vocab = []\n",
    "    embed = []\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab.append(row[0])\n",
    "            embed.append(row[1:])\n",
    "    embed = np.asarray(embed)\n",
    "    return vocab, embed\n",
    "\n",
    "\n",
    "# Load the GloVe vectors into numpy\n",
    "glove_filepath = os.path.join('datasets', 'glove.6B.50d.txt')\n",
    "vocab, embed = load_embeddings(glove_filepath)\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = len(embed[0])\n",
    "assert vocab_size > 0, \"The vocabulary shouldn't be empty; did you download the GloVe weights?\"\n",
    "print('Loaded %d %d-dimensional embeddings.' % (vocab_size, embed_dim))\n",
    "\n",
    "# word2id = {}\n",
    "# id2word = vocab\n",
    "# for i, w in enumerate(id2word):\n",
    "#     word2id[w] = i\n",
    "\n",
    "# Ops to load the embeddings into TensorFlow\n",
    "# embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embed_dim]),\n",
    "#                         trainable=False, name=\"embedding\")\n",
    "# embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embed_dim])\n",
    "# embedding_init = embedding.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO pick general categories later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_words(words):\n",
    "    word_embeds = []\n",
    "    fin_words = []\n",
    "    for i in range(400000):\n",
    "        for word in words:\n",
    "            if word == vocab[i]: \n",
    "                print('Word {} is in the dataset'.format(vocab[i]))\n",
    "                fin_words.append(word)\n",
    "                word_embeds.append(embed[i])\n",
    "                words.remove(vocab[i])\n",
    "    return fin_words, word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word police is in the dataset\n",
      "Word meeting is in the dataset\n",
      "Word election is in the dataset\n",
      "Word budget is in the dataset\n"
     ]
    }
   ],
   "source": [
    "general_categories = ['police', 'budget', 'meeting', 'election']\n",
    "category_words, category_embeds = get_embed_for_words(general_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embeds = np.vstack(tuple(category_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_repeats(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        all_cat = f.readline().split(',')\n",
    "    return all_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_phrases = load_with_repeats('../categories.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words = []\n",
    "for phrase in tag_phrases: \n",
    "    for x in phrase.strip().split(' '):\n",
    "        tag_words.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word police is in the dataset\n",
      "Word police is in the dataset\n",
      "Word police is in the dataset\n",
      "Word police is in the dataset\n",
      "Word business is in the dataset\n",
      "Word law is in the dataset\n",
      "Word service is in the dataset\n",
      "Word plan is in the dataset\n",
      "Word water is in the dataset\n",
      "Word age is in the dataset\n",
      "Word age is in the dataset\n",
      "Word population is in the dataset\n",
      "Word community is in the dataset\n",
      "Word race is in the dataset\n",
      "Word race is in the dataset\n",
      "Word action is in the dataset\n",
      "Word education is in the dataset\n",
      "Word energy is in the dataset\n",
      "Word energy is in the dataset\n",
      "Word reports is in the dataset\n",
      "Word violence is in the dataset\n",
      "Word budget is in the dataset\n",
      "Word stop is in the dataset\n",
      "Word management is in the dataset\n",
      "Word cases is in the dataset\n",
      "Word gas is in the dataset\n",
      "Word gas is in the dataset\n",
      "Word gas is in the dataset\n",
      "Word justice is in the dataset\n",
      "Word data is in the dataset\n",
      "Word data is in the dataset\n",
      "Word green is in the dataset\n",
      "Word green is in the dataset\n",
      "Word income is in the dataset\n",
      "Word records is in the dataset\n",
      "Word natural is in the dataset\n",
      "Word natural is in the dataset\n",
      "Word natural is in the dataset\n",
      "Word reason is in the dataset\n",
      "Word crime is in the dataset\n",
      "Word crime is in the dataset\n",
      "Word crime is in the dataset\n",
      "Word housing is in the dataset\n",
      "Word housing is in the dataset\n",
      "Word search is in the dataset\n",
      "Word criminal is in the dataset\n",
      "Word businesses is in the dataset\n",
      "Word traffic is in the dataset\n",
      "Word census is in the dataset\n",
      "Word census is in the dataset\n",
      "Word vehicle is in the dataset\n",
      "Word vehicle is in the dataset\n",
      "Word climate is in the dataset\n",
      "Word forest is in the dataset\n",
      "Word urban is in the dataset\n",
      "Word transportation is in the dataset\n",
      "Word statistics is in the dataset\n",
      "Word enforcement is in the dataset\n",
      "Word enforcement is in the dataset\n",
      "Word electricity is in the dataset\n",
      "Word electricity is in the dataset\n",
      "Word electricity is in the dataset\n",
      "Word tree is in the dataset\n",
      "Word assault is in the dataset\n",
      "Word ticket is in the dataset\n",
      "Word occupation is in the dataset\n",
      "Word employee is in the dataset\n",
      "Word incidents is in the dataset\n",
      "Word alcohol is in the dataset\n",
      "Word consumption is in the dataset\n",
      "Word customer is in the dataset\n",
      "Word gender is in the dataset\n",
      "Word citizenship is in the dataset\n",
      "Word salaries is in the dataset\n",
      "Word theft is in the dataset\n",
      "Word certified is in the dataset\n",
      "Word relation is in the dataset\n",
      "Word robbery is in the dataset\n",
      "Word bike is in the dataset\n",
      "Word planting is in the dataset\n",
      "Word forestry is in the dataset\n",
      "Word narcotics is in the dataset\n",
      "Word pedestrian is in the dataset\n",
      "Word co2 is in the dataset\n",
      "Word vandalism is in the dataset\n",
      "Word 311 is in the dataset\n",
      "Word crm is in the dataset\n",
      "Word ucr is in the dataset\n",
      "Word ghg is in the dataset\n",
      "Word pg&e is in the dataset\n",
      "Word ebmud is in the dataset\n"
     ]
    }
   ],
   "source": [
    "tag_words, tag_embeddings = get_embed_for_words(tag_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embed_matrix = np.vstack(tuple(tag_embeddings)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.matmul(category_embeds, tag_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 91)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tag_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(category_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embeds = category_embeds.astype(np.float32)\n",
    "tag_embed_matrix = tag_embed_matrix.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 91)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(category_embeds, tag_embed_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tag_embeds = (tag_embed_matrix * (1 / np.linalg.norm(tag_embed_matrix, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_category_embeds = category_embeds * np.expand_dims(1 / np.linalg.norm(category_embeds, axis=1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.matmul(normalized_category_embeds, normalized_tag_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_category_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sim_matrix = sim_matrix * (sim_matrix >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3417414361303979"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(masked_sim_matrix)/(masked_sim_matrix.shape[0] * masked_sim_matrix.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
